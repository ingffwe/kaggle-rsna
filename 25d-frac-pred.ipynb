{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pydicom as dicom\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torchvision as tv\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "from tqdm.notebook import tqdm\n",
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def load_dicom(path):\n",
    "    \"\"\"\n",
    "    This supports loading both regular and compressed JPEG images.\n",
    "    See the first sell with `pip install` commands for the necessary dependencies\n",
    "    \"\"\"\n",
    "    img=dicom.dcmread(path)\n",
    "    img.PhotometricInterpretation = 'YBR_FULL'\n",
    "    data = img.pixel_array\n",
    "    data = data - np.min(data)\n",
    "    if np.max(data) != 0:\n",
    "        data = data / np.max(data)\n",
    "    data=(data * 255).astype(np.uint8)\n",
    "    return data, img\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "'cuda'"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RSNA_2022_PATH = '../input/rsna-2022-cervical-spine-fracture-detection'\n",
    "TRAIN_IMAGES_PATH = f'{RSNA_2022_PATH}/train_images'\n",
    "TEST_IMAGES_PATH = f'{RSNA_2022_PATH}/test_images'\n",
    "EFFNET_MAX_TRAIN_BATCHES = 40000\n",
    "EFFNET_MAX_EVAL_BATCHES = 200\n",
    "ONE_CYCLE_MAX_LR = 0.0001\n",
    "ONE_CYCLE_PCT_START = 0.3\n",
    "SAVE_CHECKPOINT_EVERY_STEP = 1000\n",
    "EFFNET_CHECKPOINTS_PATH = '../input/rsna-2022-base-effnetv2'\n",
    "FRAC_LOSS_WEIGHT = 2.\n",
    "N_FOLDS = 5\n",
    "METADATA_PATH = '../input/vertebrae-detection-checkpoints'\n",
    "\n",
    "PREDICT_MAX_BATCHES = 1e7\n",
    "\n",
    "DEVICE='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if DEVICE == 'cuda':\n",
    "    BATCH_SIZE = 32\n",
    "else:\n",
    "    BATCH_SIZE = 2\n",
    "\n",
    "DEVICE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class EffnetDataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, path, transforms=None):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.path = path\n",
    "        self.transforms = transforms\n",
    "        self.n_25d_shift = 1\n",
    "\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "\n",
    "        path = os.path.join(self.path, self.df.iloc[i].StudyInstanceUID, f'{self.df.iloc[i].Slice}.dcm')\n",
    "\n",
    "\n",
    "        try:\n",
    "            img = self.load_2_5d_slice(path)\n",
    "            # Pytorch uses (batch, channel, height, width) order. Converting (height, width, channel) -> (channel, height, width)\n",
    "            # img = np.transpose(img, (2, 0, 1))\n",
    "            if self.transforms is not None:\n",
    "                img = self.transforms(image=img)['image']\n",
    "            img = img.to(dtype=torch.float16)\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "            return None\n",
    "\n",
    "        if 'C1_fracture' in self.df:\n",
    "            frac_targets = torch.as_tensor(self.df.iloc[i][['C1_fracture', 'C2_fracture', 'C3_fracture', 'C4_fracture',\n",
    "                                                            'C5_fracture', 'C6_fracture', 'C7_fracture']].astype(\n",
    "                'float32').values)\n",
    "            vert_targets = torch.as_tensor(\n",
    "                self.df.iloc[i][['C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7']].astype('float32').values)\n",
    "            frac_targets = frac_targets * vert_targets  # we only enable targets that are visible on the current slice\n",
    "            return img, frac_targets, vert_targets\n",
    "        return img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def load_2_5d_slice(self, middle_img_path):\n",
    "        #### 步骤1: 获取中间图片的基本信息\n",
    "        #### eg: middle_img_path: '5.dcm'\n",
    "        middle_slice_num = os.path.basename(middle_img_path).split('.')[0]  # eg: 5\n",
    "        middle_str = str(middle_slice_num)\n",
    "        # img = load_dicom(middle_img_path)[0]\n",
    "\n",
    "        new_25d_imgs = []\n",
    "\n",
    "        ##### 步骤2：按照左右n_25d_shift数量进行填充，如果没有相应图片填充为Nan.\n",
    "        ##### 注：经过EDA发现同一天的所有患者图片的shape是一致的\n",
    "        for i in range(-self.n_25d_shift, self.n_25d_shift + 1):  # eg: i = {-2, -1, 0, 1, 2}\n",
    "            shift_slice_num = int(middle_slice_num) + i\n",
    "            shift_str = str(shift_slice_num)\n",
    "            shift_img_path = middle_img_path.replace(middle_str, shift_str)\n",
    "\n",
    "            if os.path.exists(shift_img_path):\n",
    "                shift_img = load_dicom(shift_img_path)[0]\n",
    "                new_25d_imgs.append(shift_img)\n",
    "            else:\n",
    "                new_25d_imgs.append(None)\n",
    "\n",
    "        ##### 步骤3：从中心开始往外循环，依次填补None的值\n",
    "        ##### eg: n_25d_shift = 2, 那么形成5个channel, idx为[0, 1, 2, 3, 4], 所以依次处理的idx为[1, 3, 0, 4]\n",
    "        shift_left_idxs = []\n",
    "        shift_right_idxs = []\n",
    "        for related_idx in range(self.n_25d_shift):\n",
    "            shift_left_idxs.append(self.n_25d_shift - related_idx - 1)\n",
    "            shift_right_idxs.append(self.n_25d_shift + related_idx + 1)\n",
    "\n",
    "        for left_idx, right_idx in zip(shift_left_idxs, shift_right_idxs):\n",
    "            if new_25d_imgs[left_idx] is None:\n",
    "                new_25d_imgs[left_idx] = new_25d_imgs[left_idx + 1]\n",
    "            if new_25d_imgs[right_idx] is None:\n",
    "                new_25d_imgs[right_idx] = new_25d_imgs[right_idx - 1]\n",
    "        try:\n",
    "            new_25d_imgs = np.stack(new_25d_imgs, axis=2).astype('float32')  # [w, h, c]\n",
    "            mx_pixel = new_25d_imgs.max()\n",
    "            if mx_pixel != 0:\n",
    "                new_25d_imgs /= mx_pixel\n",
    "        except:\n",
    "            return np.zeros((512, 512, 3))\n",
    "        return new_25d_imgs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "                 StudyInstanceUID  Slice  ImageHeight  ImageWidth  \\\n0       1.2.826.0.1.3680043.10001      1          512         512   \n1       1.2.826.0.1.3680043.10001      2          512         512   \n2       1.2.826.0.1.3680043.10001      3          512         512   \n3       1.2.826.0.1.3680043.10001      4          512         512   \n4       1.2.826.0.1.3680043.10001      5          512         512   \n...                           ...    ...          ...         ...   \n711498   1.2.826.0.1.3680043.9997    251          512         512   \n711499   1.2.826.0.1.3680043.9997    252          512         512   \n711500   1.2.826.0.1.3680043.9997    253          512         512   \n711501   1.2.826.0.1.3680043.9997    254          512         512   \n711502   1.2.826.0.1.3680043.9997    255          512         512   \n\n        SliceThickness  ImagePositionPatient_x  ImagePositionPatient_y  \\\n0                0.625                 -52.308                 -27.712   \n1                0.625                 -52.308                 -27.712   \n2                0.625                 -52.308                 -27.712   \n3                0.625                 -52.308                 -27.712   \n4                0.625                 -52.308                 -27.712   \n...                ...                     ...                     ...   \n711498           0.625                 -55.200                 -24.600   \n711499           0.625                 -55.200                 -24.600   \n711500           0.625                 -55.200                 -24.600   \n711501           0.625                 -55.200                 -24.600   \n711502           0.625                 -55.200                 -24.600   \n\n        ImagePositionPatient_z  C1  C2  ...  C7  patient_overall  C1_fracture  \\\n0                        7.282   0   0  ...   0                0            0   \n1                        6.657   0   0  ...   0                0            0   \n2                        6.032   0   0  ...   0                0            0   \n3                        5.407   0   0  ...   0                0            0   \n4                        4.782   0   0  ...   0                0            0   \n...                        ...  ..  ..  ...  ..              ...          ...   \n711498                -187.750   0   0  ...   0                0            0   \n711499                -188.375   0   0  ...   0                0            0   \n711500                -189.000   0   0  ...   0                0            0   \n711501                -189.625   0   0  ...   0                0            0   \n711502                -190.250   0   0  ...   0                0            0   \n\n        C2_fracture  C3_fracture  C4_fracture  C5_fracture  C6_fracture  \\\n0                 0            0            0            0            0   \n1                 0            0            0            0            0   \n2                 0            0            0            0            0   \n3                 0            0            0            0            0   \n4                 0            0            0            0            0   \n...             ...          ...          ...          ...          ...   \n711498            0            0            0            0            0   \n711499            0            0            0            0            0   \n711500            0            0            0            0            0   \n711501            0            0            0            0            0   \n711502            0            0            0            0            0   \n\n        C7_fracture  split  \n0                 0    3.0  \n1                 0    3.0  \n2                 0    3.0  \n3                 0    3.0  \n4                 0    3.0  \n...             ...    ...  \n711498            0    1.0  \n711499            0    1.0  \n711500            0    1.0  \n711501            0    1.0  \n711502            0    1.0  \n\n[711503 rows x 24 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>StudyInstanceUID</th>\n      <th>Slice</th>\n      <th>ImageHeight</th>\n      <th>ImageWidth</th>\n      <th>SliceThickness</th>\n      <th>ImagePositionPatient_x</th>\n      <th>ImagePositionPatient_y</th>\n      <th>ImagePositionPatient_z</th>\n      <th>C1</th>\n      <th>C2</th>\n      <th>...</th>\n      <th>C7</th>\n      <th>patient_overall</th>\n      <th>C1_fracture</th>\n      <th>C2_fracture</th>\n      <th>C3_fracture</th>\n      <th>C4_fracture</th>\n      <th>C5_fracture</th>\n      <th>C6_fracture</th>\n      <th>C7_fracture</th>\n      <th>split</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.2.826.0.1.3680043.10001</td>\n      <td>1</td>\n      <td>512</td>\n      <td>512</td>\n      <td>0.625</td>\n      <td>-52.308</td>\n      <td>-27.712</td>\n      <td>7.282</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.2.826.0.1.3680043.10001</td>\n      <td>2</td>\n      <td>512</td>\n      <td>512</td>\n      <td>0.625</td>\n      <td>-52.308</td>\n      <td>-27.712</td>\n      <td>6.657</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.2.826.0.1.3680043.10001</td>\n      <td>3</td>\n      <td>512</td>\n      <td>512</td>\n      <td>0.625</td>\n      <td>-52.308</td>\n      <td>-27.712</td>\n      <td>6.032</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.2.826.0.1.3680043.10001</td>\n      <td>4</td>\n      <td>512</td>\n      <td>512</td>\n      <td>0.625</td>\n      <td>-52.308</td>\n      <td>-27.712</td>\n      <td>5.407</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.2.826.0.1.3680043.10001</td>\n      <td>5</td>\n      <td>512</td>\n      <td>512</td>\n      <td>0.625</td>\n      <td>-52.308</td>\n      <td>-27.712</td>\n      <td>4.782</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>711498</th>\n      <td>1.2.826.0.1.3680043.9997</td>\n      <td>251</td>\n      <td>512</td>\n      <td>512</td>\n      <td>0.625</td>\n      <td>-55.200</td>\n      <td>-24.600</td>\n      <td>-187.750</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>711499</th>\n      <td>1.2.826.0.1.3680043.9997</td>\n      <td>252</td>\n      <td>512</td>\n      <td>512</td>\n      <td>0.625</td>\n      <td>-55.200</td>\n      <td>-24.600</td>\n      <td>-188.375</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>711500</th>\n      <td>1.2.826.0.1.3680043.9997</td>\n      <td>253</td>\n      <td>512</td>\n      <td>512</td>\n      <td>0.625</td>\n      <td>-55.200</td>\n      <td>-24.600</td>\n      <td>-189.000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>711501</th>\n      <td>1.2.826.0.1.3680043.9997</td>\n      <td>254</td>\n      <td>512</td>\n      <td>512</td>\n      <td>0.625</td>\n      <td>-55.200</td>\n      <td>-24.600</td>\n      <td>-189.625</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>711502</th>\n      <td>1.2.826.0.1.3680043.9997</td>\n      <td>255</td>\n      <td>512</td>\n      <td>512</td>\n      <td>0.625</td>\n      <td>-55.200</td>\n      <td>-24.600</td>\n      <td>-190.250</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>711503 rows × 24 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(f'{RSNA_2022_PATH}/train.csv')\n",
    "df_train_slices = pd.read_csv(r'W:\\PycharmProjects\\kaggle-RSNA\\input\\rsna-2022-spine-fracture-detection-metadata\\train_segmented.csv')\n",
    "c1c7 = [f'C{i}' for i in range(1, 8)]\n",
    "df_train_slices[c1c7] = (df_train_slices[c1c7] > 0.5).astype(int)\n",
    "df_train = df_train_slices.set_index('StudyInstanceUID').join(df_train.set_index('StudyInstanceUID'),\n",
    "                                                              rsuffix='_fracture').reset_index().copy()\n",
    "df_train = df_train.query('StudyInstanceUID != \"1.2.826.0.1.3680043.20574\"').reset_index(drop=True)\n",
    "\n",
    "split = GroupKFold(N_FOLDS)\n",
    "for k, (_, test_idx) in enumerate(split.split(df_train, groups=df_train.StudyInstanceUID)):\n",
    "    df_train.loc[test_idx, 'split'] = k\n",
    "df_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "\n",
    "def weighted_loss(y_pred_logit, y, reduction='mean', verbose=False):\n",
    "    \"\"\"\n",
    "    Weighted loss\n",
    "    We reuse torch.nn.functional.binary_cross_entropy_with_logits here. pos_weight and weights combined give us necessary coefficients described in https://www.kaggle.com/competitions/rsna-2022-cervical-spine-fracture-detection/discussion/340392\n",
    "\n",
    "    See also this explanation: https://www.kaggle.com/code/samuelcortinhas/rsna-fracture-detection-in-depth-eda/notebook\n",
    "    \"\"\"\n",
    "\n",
    "    neg_weights = (torch.tensor([7., 1, 1, 1, 1, 1, 1, 1]) if y_pred_logit.shape[-1] == 8 else torch.ones(y_pred_logit.shape[-1])).to(DEVICE)\n",
    "    pos_weights = (torch.tensor([14., 2, 2, 2, 2, 2, 2, 2]) if y_pred_logit.shape[-1] == 8 else torch.ones(y_pred_logit.shape[-1]) * 2.).to(DEVICE)\n",
    "\n",
    "    loss = torch.nn.functional.binary_cross_entropy_with_logits(\n",
    "        y_pred_logit,\n",
    "        y,\n",
    "        reduction='none',\n",
    "    )\n",
    "\n",
    "    if verbose:\n",
    "        print('loss', loss)\n",
    "\n",
    "    pos_weights = y * pos_weights.unsqueeze(0)\n",
    "    neg_weights = (1 - y) * neg_weights.unsqueeze(0)\n",
    "    all_weights = pos_weights + neg_weights\n",
    "\n",
    "    if verbose:\n",
    "        print('all weights', all_weights)\n",
    "\n",
    "    loss *= all_weights\n",
    "    if verbose:\n",
    "        print('weighted loss', loss)\n",
    "\n",
    "    norm = torch.sum(all_weights, dim=1).unsqueeze(1)\n",
    "    if verbose:\n",
    "        print('normalization factors', norm)\n",
    "\n",
    "    loss /= norm\n",
    "    if verbose:\n",
    "        print('normalized loss', loss)\n",
    "\n",
    "    loss = torch.sum(loss, dim=1)\n",
    "    if verbose:\n",
    "        print('summed up over patient_overall-C1-C7 loss', loss)\n",
    "\n",
    "    if reduction == 'mean':\n",
    "        return torch.mean(loss)\n",
    "    return loss\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class EffnetModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        effnet = tv.models.efficientnet_v2_s()\n",
    "        self.model = create_feature_extractor(effnet, ['flatten'])\n",
    "        self.nn_fracture = torch.nn.Sequential(\n",
    "            torch.nn.Linear(1280, 7),\n",
    "        )\n",
    "        self.nn_vertebrae = torch.nn.Sequential(\n",
    "            torch.nn.Linear(1280, 7),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # returns logits\n",
    "        x = self.model(x)['flatten']\n",
    "        return self.nn_fracture(x), self.nn_vertebrae(x)\n",
    "\n",
    "    def predict(self, x):\n",
    "        frac, vert = self.forward(x)\n",
    "        return torch.sigmoid(frac), torch.sigmoid(vert)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def filter_nones(b):\n",
    "    return torch.utils.data.default_collate([v for v in b if v is not None])\n",
    "\n",
    "def evaluate_effnet(model: EffnetModel, dl_test, max_batches=PREDICT_MAX_BATCHES, shuffle=False):\n",
    "    torch.manual_seed(42)\n",
    "    model = model.to(DEVICE)\n",
    "    # dl_test = torch.utils.data.DataLoader(ds, batch_size=BATCH_SIZE, shuffle=shuffle, num_workers=os.cpu_count(),\n",
    "                                          # collate_fn=filter_nones)\n",
    "    pred_frac = []\n",
    "    pred_vert = []\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        frac_losses = []\n",
    "\n",
    "        with tqdm(dl_test, desc='Eval') as progress:\n",
    "            for i, (X, y_frac, y_vert) in enumerate(progress):\n",
    "                with autocast():\n",
    "                    y_frac_pred = model(X.to(DEVICE))\n",
    "                    frac_loss = weighted_loss(y_frac_pred, y_frac.to(DEVICE)).item()\n",
    "                    pred_frac.append(torch.sigmoid(y_frac_pred))\n",
    "                    pred_vert.append(y_vert)\n",
    "                    frac_losses.append(frac_loss)\n",
    "\n",
    "                if i >= max_batches:\n",
    "                    break\n",
    "        return np.mean(frac_losses), torch.concat(pred_frac).cpu().numpy(),torch.concat(pred_vert).cpu().numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "Eval:   0%|          | 0/2223 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "58ca8af340de42c98bf96a542794a682"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fold = 0\n",
    "ds_train = EffnetDataSet(df_train.query('split != @fold'), TRAIN_IMAGES_PATH, A.Compose([\n",
    "                      A.Resize(512, 512),\n",
    "                      A.HorizontalFlip(p=0.5),\n",
    "                      # # A.RandomContrast(p=0.5),\n",
    "                      # # A.RandomBrightness(p=0.5),\n",
    "                      # A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "                      # # A.RandomBrightness(limit=2, p=0.5),\n",
    "                      # A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.05, rotate_limit=10, p=0.2),\n",
    "                      #\n",
    "                      # A.OneOf([\n",
    "                      #     A.MotionBlur(p=0.2),\n",
    "                      #     A.MedianBlur(blur_limit=3, p=0.1),\n",
    "                      #     A.Blur(blur_limit=3, p=0.1),\n",
    "                      # ], p=0.5),\n",
    "                      ToTensorV2(),\n",
    "\n",
    "                      ])\n",
    "                  )\n",
    "\n",
    "ds_val = EffnetDataSet(df_train.query('split == @fold'), TRAIN_IMAGES_PATH,A.Compose([\n",
    "                      A.Resize(384, 384),\n",
    "                      # A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "                      ToTensorV2(),\n",
    "                      ])\n",
    "                  )\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(ds_train, batch_size=16, shuffle=True, num_workers=10)\n",
    "val_loader = torch.utils.data.DataLoader(ds_val, batch_size=64, shuffle=False, num_workers=0)\n",
    "\n",
    "model = torch.jit.load('../output/ckpt-25d-frac/ENV2_384_fold0_loss0.0404.pth')\n",
    "model = model.to('cuda')\n",
    "\n",
    "frac_loss, effnet_pred_frac,effnet_pred_vert = evaluate_effnet(model, val_loader, PREDICT_MAX_BATCHES)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "effnet_pred_frac"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_pred = []\n",
    "fold = 0\n",
    "effnet_pred_vert = effnet_pred_frac\n",
    "df_effnet_pred = pd.DataFrame(data=np.concatenate([effnet_pred_frac, effnet_pred_vert], axis=1),\n",
    "                              columns=[f'C{i}_effnet_frac' for i in range(1, 8)] +\n",
    "                                      [f'C{i}_effnet_vert' for i in range(1, 8)])\n",
    "df = pd.concat([df_train.query('split == @fold').head(len(df_effnet_pred)).reset_index(drop=True), df_effnet_pred],axis=1).sort_values(['StudyInstanceUID', 'Slice'])\n",
    "df_pred.append(df)\n",
    "df_pred = pd.concat(df_pred)\n",
    "df_pred"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df_vert_slices = pd.read_csv(r'W:\\PycharmProjects\\kaggle-RSNA\\input\\rsna-2022-spine-fracture-detection-metadata\\train_segmented.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # 把traget_segmented里的vert拿来用\n",
    "# row_num = 0\n",
    "# c1c7 = [f'C{i}' for i in range(1, 8)]\n",
    "# c1c7_vert = [f'C{i}_effnet_vert' for i in range(1, 8)]\n",
    "# for row in tqdm(df_pred.iterrows()):\n",
    "#     studyInstanceUID = row[1][0]\n",
    "#     slic = row[1][1]\n",
    "#     df_vert_slice = df_vert_slices.loc[df_vert_slices['StudyInstanceUID'] == studyInstanceUID]\n",
    "#     df_vert_slice = df_vert_slice.loc[df_vert_slice['Slice'] == slic]\n",
    "#     slice_c1c7 = df_vert_slice[c1c7]\n",
    "#     df_pred.loc[row_num,[f'C{i}_effnet_vert' for i in range(1, 8)]] = np.array(slice_c1c7).tolist()[0]\n",
    "#     row_num+=1\n",
    "#\n",
    "# df_pred"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "target_cols = ['patient_overall'] + [f'C{i}_fracture' for i in range(1, 8)]\n",
    "frac_cols = [f'C{i}_effnet_frac' for i in range(1, 8)]\n",
    "vert_cols = [f'C{i}_effnet_vert' for i in range(1, 8)]\n",
    "\n",
    "\n",
    "def patient_prediction(df):\n",
    "    c1c7 = np.average(df[frac_cols].values, axis=0, weights=df[vert_cols].values)\n",
    "    pred_patient_overall = 1 - np.prod(1 - c1c7)\n",
    "    return np.concatenate([[pred_patient_overall], c1c7])\n",
    "\n",
    "df_patient_pred = df_pred.groupby('StudyInstanceUID').apply(lambda df: patient_prediction(df)).to_frame('pred').join(df_pred.groupby('StudyInstanceUID')[target_cols].mean())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions = np.stack(df_patient_pred.pred.values.tolist())\n",
    "predictions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "targets = df_patient_pred[target_cols].values\n",
    "targets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('CV score:', weighted_loss(torch.logit(torch.as_tensor(predictions)).to(DEVICE), torch.as_tensor(targets).to(DEVICE)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}